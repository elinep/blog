<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Untitled RSS Feed]]></title><description><![CDATA[Untitled RSS Feed]]></description><link>https://elinep.github.io/blog</link><generator>RSS for Node</generator><lastBuildDate>Wed, 14 Jun 2017 07:55:53 GMT</lastBuildDate><atom:link href="https://elinep.github.io/blog/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Neural Networks from scratch]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In the <a href="https://elinep.github.io/blog/2017/06/12/neural_networks_training_basics.html">previous post</a>
we saw the theory about neural network training. It&#8217;s time to practice by writing
a basic implementation from scratch in python. The code is available
<a href="https://github.com/elinep/nn_from_scratch/blob/master/scratch_nn.py">here</a>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_objectives">Objectives</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Few goals for our tiny neural network framework :</p>
</div>
<div class="ulist">
<ul>
<li>
<p>the network architecture must be easy to define (number of layers, neuron
per layers, &#8230;&#8203;)</p>
</li>
<li>
<p>we will focus on fully connected network</p>
</li>
<li>
<p>it must be trainable</p>
</li>
<li>
<p>no use of specialized package like tensorflow</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To test it, we will train the network to approximate a function. I arbitrarily
choose the next one:</p>
</div>
<div class="paragraph text-center">
<p>\(y = f(x_0,x_1) = x_0 x_1 + 2x_0 + 1\)</p>
</div>
<div class="paragraph text-center">
<p>\((x_0,x_1) \in \mathbb{R}^2\) and \(y \in \mathbb{R}\)</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://elinep.github.io/blog/images/2017-06-13-neural_network_from_scratch/fig1.png" alt="fig1.png">
</div>
<div class="title">Figure 1. Our objective function</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_code">Code</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_block_class">Block class</h3>
<div class="sect3">
<h4 id="_abstractblock_class">AbstractBlock class</h4>
<div class="paragraph">
<p>We start by defining an abstract class which will be the base class for all the
nodes (neurons, activation function, loss function) in the network.</p>
</div>
<div class="paragraph">
<p>This class defines 3 methods:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>forward()</strong>, receives input data and return an output, used for <strong>forward
propagation</strong></p>
</li>
<li>
<p><strong>backward()</strong> receives output gradient and return input gradient, used for
<strong>backward propagation</strong></p>
</li>
<li>
<p><strong>update()</strong> called during <strong>weights update</strong>. This function is optional because
blocks like loss/activation functions do not have weights.</p>
</li>
</ul>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">class AbstractBlock(object):
    '''
    Abstract class of a block
    a block has multiple input, one output.
    a block must have forward() and backward() methods
    update() method is optional
    '''
    def __init__(self, input_dim):
        self.input_dim = input_dim
        # input data
        self.x = np.zeros(input_dim)
        # output data
        self.y = 0
        # gradient data
        self.grad_x = np.zeros(input_dim)

    '''
    forward pass
    input_data is a vector, each element corresponds to a neuron input
    '''
    def forward(self, input_data):
        raise NotImplementedError('forward not implemented')

    '''
    backward pass
    output_gradient is a scalar
    '''
    def backward(self, output_gradient):
        raise NotImplementedError('backward not implemented')

    '''
    trigger weight update
    '''
    def update(self, learn_rate):
        pass</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
forward() and backward() do not have default implementation as we don&#8217;t know
yet what an AbstractBlock computes.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_neuron_class">Neuron class</h4>
<div class="paragraph">
<p>Now we define the <strong>Neuron</strong> class which inherits from <strong>AbstractBlock</strong> and implements
the neuron transformation:</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">class Neuron(AbstractBlock):
    '''
    Neuron class computing a weighted sum of its input
    '''
    def __init__(self, input_dim):
        super(Neuron, self).__init__(input_dim)
        # initialize random weight
        self.weight = np.random.normal(loc=0.0, scale=0.1, size=(input_dim))
        # initialize bias
        self.bias = 0.0
        # init weight and bias gradient
        self.grad_weight = np.zeros(input_dim)
        self.grad_bias = 0.0

    def forward(self, input_data):
        # save input data
        self.x = input_data
        # compute output
        self.y = np.dot(self.x, self.weight) + self.bias
        return self.y

    def backward(self, output_gradient):
        self.grad_bias = output_gradient
        self.grad_weight = self.x * output_gradient
        self.grad_x = self.weight * output_gradient
        return self.grad_x

    def update(self, learn_rate):
        self.weight += -learn_rate * self.grad_weight
        self.bias += -learn_rate * self.grad_bias</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Neuron</strong> has some extra initialization compared to <strong>AbstractBlock</strong> since it has
weight and bias parameters. Weights are randomly initialized while bias is set to
0</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">def __init__(self, input_dim):
    super(Neuron, self).__init__(input_dim)
    # initialize random weight
    self.weight = np.random.normal(loc=0.0, scale=0.1, size=(input_dim))
    # initialize bias
    self.bias = 0.0
    # init weight and bias gradient
    self.grad_weight = np.zeros(input_dim)
    self.grad_bias = 0.0</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>forward()</strong> implements the neuron definition:</p>
</div>
<div class="paragraph text-center">
<p>\(Y = f_{neuron}(X,W,B) = \sum\limits_{i=0}^{N}{(X_i \cdot W_i)}+B\)</p>
</div>
<div class="paragraph">
<p><strong>backward()</strong> compute the partial derivative with respect to the parameters (weight, bias)
and the input data (x) and multiply the result by the output gradient (chain rule):</p>
</div>
<div class="paragraph text-center">
<p>\(G_{W_i} = G_Y \cdot \frac{\partial F_{neuron}(X,W,B)}{\partial W_i}
=G_Y \cdot X_i\)</p>
</div>
<div class="paragraph text-center">
<p>\(G_{B} = G_Y \cdot \frac{\partial F_{neuron}(X,W,B)}{\partial W_B}
=G_Y \cdot 1\)</p>
</div>
<div class="paragraph text-center">
<p>\(G_{X_i} = G_Y \cdot \frac{\partial F_{neuron}(X,W,B)}{\partial W_i}
=G_Y \cdot W_i\)</p>
</div>
<div class="paragraph">
<p><strong>update()</strong> implements the gradient descent by incrementing the neuron parameter
with a fraction of the gradient:</p>
</div>
<div class="paragraph text-center">
<p>\(W_i = W_i - \lambda G_{W_i}\)</p>
</div>
<div class="paragraph text-center">
<p>\(B = B - \lambda G_{B}\)</p>
</div>
</div>
<div class="sect3">
<h4 id="_activation_class">Activation class</h4>
<div class="paragraph">
<p>The activation function is placed right after a neuron. It is a
\(\mathbb{R} \to \mathbb{R}\) as opposite to a neuron which is
\(\mathbb{R}^N \to \mathbb{R}\). It doesn&#8217;t have any parameters. Here we implement
the popular <strong>relu</strong>:</p>
</div>
<div class="paragraph text-center">
<p>\(Y = f_{relu}(X) = \max(0,X)\)</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">class Relu(AbstractBlock):
    '''
    Relu activation function block
    '''
    def __init__(self):
        super(Relu, self).__init__(1)

    def forward(self, input_data):
        # save input data
        self.x = input_data
        # compute output
        self.y = max(0.0, self.x)
        return self.y

    def backward(self, output_gradient):
        if self.x &gt; 0:
            self.grad_x = output_gradient
        else:
            self.grad_x = 0.0
        return self.grad_x</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>backward()</strong> is straight forward to implement. The function varies only with its
scalar input:</p>
</div>
<div class="paragraph text-center">
<p>\(G_X = G_Y \cdot \frac{\partial f_{relu}(X)}{\partial X}
=G_Y \cdot
\Big\{
\begin{matrix}
1 &amp; Y&gt;0 \\
0 &amp; Y \leqslant 0
\end{matrix}\)</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
strictly speaking the \(f_{relu}(X)\) can not be derived for \(X=0\)
In practice it is not annoying as it affects only one single point and we arbitrarily
choose to extend the derivative from \(X&lt;0\) or \(X&gt;0\)
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p><strong>update()</strong> class is not redefined as there are no weights to optimize in an activation
function</p>
</div>
</div>
<div class="sect3">
<h4 id="_loss_class">Loss class</h4>
<div class="paragraph">
<p>A loss function computes the error between the output of the network and the expected
output from the training data. We need a way to provide the expected data to a loss <strong>block</strong>.
We define an <strong>AbstractLoss</strong> class for this purpose</p>
</div>
<div class="sect4">
<h5 id="_abstractloss_class">AbstractLoss class</h5>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">class AbstractLoss(AbstractBlock):
    '''
    Abstract class of a loss function
    A loss function is a "block" with a setExpectedData() method to set
    the ground truth result of a neural network
    '''
    def __init__(self, input_dim):
        super(AbstractLoss, self).__init__(input_dim)
        self.expected_data = np.zeros(input_dim)

    def setExpectedData(self, expected_data):
        self.expected_data = expected_data</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>AbstractLoss</strong> only adds a new method <strong>setExpectedData()</strong> to the <strong>AbstractBlock</strong>.
<strong>setExpectedData()</strong> is used to pass the ground truth output from the training data.
The expected data are stored to be used later by the <strong>forward()</strong> method.</p>
</div>
</div>
<div class="sect4">
<h5 id="_lossl2_class">LossL2 class</h5>
<div class="paragraph">
<p>Now we implement a real loss function, the \(L2\) norm:</p>
</div>
<div class="paragraph text-center">
<p>\(L(Y,\tilde{Y}) = \sum{(Y_i-\tilde{Y_i})^2}\)</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">class LossL2(AbstractLoss):
    '''
    A loss function that compute the L2 norm between its input_data and
    ground truth data
    '''
    def __init__(self, input_dim):
        super(LossL2, self).__init__(input_dim)

    def forward(self, input_data):
        # save input data
        self.x = input_data
        # compute L2 distance between input data and expected input data
        self.y = np.sum(np.square(self.x - self.expected_data))
        return self.y

    def backward(self):
        self.grad_x = 2 * (self.x - self.expected_data)
        return self.grad_x</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>backward()</strong> takes no argument as we compute the gradient with respect to the
error. The loss function is the first stage for
<strong>backward propagation</strong>. We only compute the gradient with respect to \(Y\) as
\(\tilde{Y}\) do not varies with the weights of the network.</p>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_network_class">Network class</h3>
<div class="paragraph">
<p>We have all the elementary blocks we need. Let&#8217;s write a new class to manage them.
The <strong>Network</strong> class will allow us to build/train and run a network.</p>
</div>
</div>
<div class="sect2">
<h3 id="_building_network">building network</h3>
<div class="paragraph">
<p>First let&#8217;s focus on the functions to build the network</p>
</div>
<div class="paragraph">
<p>When instantiating the <strong>Neuron</strong> we provide the dimensions of our input and output
data. These informations are used to determine the number of inputs per neuron of
the first layer and the number of neurons of the last layer.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">def __init__(self, input_dim, output_dim):
    # data dimensions
    self._input_dim = input_dim
    self._output_dim = output_dim
    # the neuron layer stack
    self._network = []
    # helper to know the number of input for the next neuron layer
    self._last_input_dim = input_dim
    # the loss function will be added later
    self._loss = None
    # current output of the network
    self._last_network_output = np.zeros(output_dim)</code></pre>
</div>
</div>
<div class="paragraph">
<p>We define a method to add a layer of neuron. This method takes an <strong>AbstractBlock</strong>
as first argument which defines the neuron transformation. The second argument is
the number of neurons for the layer.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">'''
Add a layer of neuron fully connected to the previous layer
'''
def addNeuronlayer(self, NeuronClassType, size):
    layer = {
        'data' : [ NeuronClassType(self._last_input_dim) for i in range(size)],
        'input_dim' : self._last_input_dim,
        'output_dim' : size,
        'type' : 'neuron'
    }
    self._network.append(layer)
    self._last_input_dim = size</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>addNeuronlayer()</strong> method adds a fully connected layer. Each neuron of the layer
considers all the output of the previous layer as input.
We instantiate <strong>size</strong> neurons, add some metadata and stack them into the
<strong>self._network</strong> array.</p>
</div>
<div class="paragraph">
<p>We write a similar method to add an activation layer.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">'''
Add an activation layer (one activation block per block of the previous layer)
'''
def addActivationLayer(self, ActivationClassType):
    layer = {
        'data' : [ ActivationClassType() for i in range(self._last_input_dim)],
        'input_dim' : self._last_input_dim,
        'output_dim' : self._last_input_dim,
        'type' : 'activation'
    }
    self._network.append(layer)
    pass</code></pre>
</div>
</div>
<div class="paragraph">
<p>Since activation function are placed after each neuron, there is no <strong>size</strong> argument.
We create as many <strong>ActivationClass</strong> as neurons in the previous layer.</p>
</div>
</div>
<div class="sect2">
<h3 id="_set_loss">set loss</h3>
<div class="paragraph">
<p>The user can set the loss function thanks to the next method</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">'''
Set the loss function for the optimization
'''
def setLoss(self, LossClass):
    self._loss = LossClass(self._output_dim)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_forward_backward_propagation_through_the_network">forward/backward propagation through the network</h3>
<div class="paragraph">
<p>Next method propagates data by iterating over layers and blocks. It returns the
network output.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">'''
Process input_data by the network
'''
def _forward(self, input_data):
    layer_input = input_data
    layer_i = 0
    for layer in self._network:
        # temporary layer output
        layer_output = np.zeros(layer['output_dim'])

        if layer['type'] == 'neuron':
            # feed each block of the layer with the input data
            b_i = 0
            for b in layer['data']:
                layer_output[b_i] = b.forward(layer_input)
                b_i += 1
        elif layer['type'] == 'activation':
            # one to one connection between input data and output data elements
            b_i = 0
            for b in layer['data']:
                layer_output[b_i] = b.forward(layer_input[b_i])
                b_i += 1
        else:
            raise ValueError('unknow layer type %s, can not run the network' % layer['type'])

        # current output is the input of the next layer
        layer_input = layer_output
        layer_i += 1

    return layer_output</code></pre>
</div>
</div>
<div class="paragraph">
<p>As we notice, there are two cases:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>layer of neuron, the entire output vector of the previous layer is fed to each
neurons since we are implementing a fully connected network.</p>
</li>
<li>
<p>layer of activation function, only the output of the matching previous layer neuron is fed
to the activation function</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The output of the current layer becomes the input of the next layer.</p>
</div>
<div class="paragraph">
<p>Similarly we have the backward propagation.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">'''
Back propagate through the entire network
'''
def _backward(self, output_grad):
    layer_output_grad = output_grad
    layer_i = 0
    for layer in reversed(self._network):
        # temporary layer input gradient
        layer_input_grad = np.zeros(layer['input_dim'])

        if layer['type'] == 'neuron':
            b_i = 0
            for b in layer['data']:
                layer_input_grad += b.backward(layer_output_grad[b_i])
                b_i += 1
        elif layer['type'] == 'activation':
            b_i = 0
            for b in layer['data']:
                layer_input_grad[b_i] = b.backward(layer_output_grad[b_i])
                b_i += 1
        else:
            raise ValueError('unknow layer type %s, can not run the network' % layer['type'])

        layer_output_grad = layer_input_grad
        layer_i += 1
    return</code></pre>
</div>
</div>
<div class="paragraph">
<p>There are also two cases:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>layer of neuron, we accumulate the gradient since the output of the previous layer is
connected to multiple neurons.</p>
</li>
<li>
<p>layer of activation function, we provide a scalar gradient and get a scalar gradient</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_training">Training</h3>
<div class="paragraph">
<p>The training iterates over a set of examples. Each example is forward propagated
through the network. The error is computed thanks to the loss block. We then back
propagate through the loss and the network. Finally we trigger a weight <strong>update()</strong>
for all <strong>block</strong> of the network. In the meantime we average the error over the whole
training set to monitor the training behavior. The full training set is presented
<strong>epoch</strong> times.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">'''
Train the network with examples
(X,Y) training data set (each row is an example)
'''
def train(self, X, Y, epoch, learn_rate):
    num_example = X.shape[1]
    loss_historic = np.zeros(epoch)
    for e in range(epoch):
        l_mean = 0.0
        for x, y in zip(X, Y):
            # do forward pass
            network_output = self._forward(x)
            # compute loss
            self._loss.setExpectedData(y)
            l_mean += self._loss.forward(network_output)
            # back propagate
            l_grad = self._loss.backward()
            self._backward(l_grad)
            # update weight
            self._update(learn_rate)
        loss_historic[e] = l_mean / num_example
        print("epoch %d/%d average loss %f" % (e, epoch, loss_historic[e]))
    return loss_historic

    '''
    Trigger a weight update for every block of the network
    '''
    def _update(self, learn_rate):
        for layer in self._network:
            for b in layer['data']:
                b.update(learn_rate)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_running">Running</h3>
<div class="paragraph">
<p>Finally a method for forward propagation only to use our network once trained.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">'''
Process a bunch of input data
'''
def run(self, input_data):
    input_data = input_data.reshape((-1, self._input_dim))
    y = np.zeros((input_data.shape[0], self._output_dim))
    i = 0
    for x in input_data:
        y[i, :] = self._forward(x)
        i += 1
    return y</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_main">Main</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The main() function prepares the training data, sets up the network, trains it
and displays the results.</p>
</div>
<div class="sect2">
<h3 id="_training_data">Training data</h3>
<div class="paragraph">
<p>We define a lambda objective function as defined in the introduction. We generate
random input data and process them with our objective function.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python"># the objective function the network has to mimic
function_to_learn = lambda x0, x1: np.add(np.multiply(x0, x1), 2 * x0) + 1

x_range = 10
# generate random input data
X_train = np.random.uniform(-x_range, x_range, (NUM_EXAMPLES, 2))
# generate matching output data
Y_train = function_to_learn(X_train[:, 0], X_train[:, 1])</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_build_model">Build model</h3>
<div class="paragraph">
<p>We instantiate a network and stack neuron layers and activation layers. Each layer
has <strong>NUM_HIDDEN_NODES</strong> neurons and there are <strong>NUM_HIDDEN_LAYER</strong> layers. We end with
a single neuron which is the output of the network.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">model = Network(2, 1)
for h in range(NUM_HIDDEN_LAYER):
    model.addNeuronlayer(Neuron, NUM_HIDDEN_NODES)
    model.addActivationLayer(Relu)
model.addNeuronlayer(Neuron, 1)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_trainning">Trainning</h3>
<div class="paragraph">
<p>Finally we set a loss function and start training.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">model.setLoss(LossL2)
loss_historic = model.train(X_train, Y_train, NUM_EPOCHS, LEARNING_RATE)</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_results">Results</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The script displays two figures. First, an overlay of the objective function and
the network function. As we can see both overlay quite nicely.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://elinep.github.io/blog/images/2017-06-13-neural_network_from_scratch/fig2.png" alt="fig2.png">
</div>
<div class="title">Figure 2. Network vs Objective function</div>
</div>
<div class="paragraph">
<p>The error also decreases nicely:</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://elinep.github.io/blog/images/2017-06-13-neural_network_from_scratch/fig3.png" alt="fig3.png">
</div>
<div class="title">Figure 3. Error over time</div>
</div>
<div class="paragraph">
<p>In the main function we have all the hyperparameters we can play with:</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">NUM_HIDDEN_NODES = 10
NUM_HIDDEN_LAYER = 2
NUM_EXAMPLES = 1000
NUM_EPOCHS = 200
LEARNING_RATE = 0.0001</code></pre>
</div>
</div>
</div>
</div>]]></description><link>https://elinep.github.io/blog/2017/06/13/neural_network_from_scratch.html</link><guid isPermaLink="true">https://elinep.github.io/blog/2017/06/13/neural_network_from_scratch.html</guid><category><![CDATA[Blog]]></category><category><![CDATA[Neural network]]></category><category><![CDATA[Python]]></category><pubDate>Tue, 13 Jun 2017 00:00:00 GMT</pubDate></item><item><title><![CDATA[Neural Networks Training: basic concepts]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>I recently started to be interested in neural networks attracted by all the
magical articles/results that flood the web.
When you see magic, you want to understand the trick. That’s my case.
A great resource to study this subject is the
<a href="https://www.youtube.com/watch?v=yp9rwI_LZX8&amp;list=PL16j5WbGpaM0_Tj8CRmurZ8Kk1gEBc7fg">Convolutional Neural Networks</a> course by Standford.
This series of videos is nice and show that neural network concepts are accessible.
There is still few points that struggled me for a while and my modest intent is
that this post freshly written after having watched the video could help for a
better understanding.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_what_is_a_neural_network">What is a neural network ?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>OK first, what kind of beast a neural network is and what’s its purpose. Well
it’s just a giant function that process some input based on some parameters to
compute some output.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://elinep.github.io/blog/images/2017-06-12-neural_networks_training_basics/fig1.png" alt="fig1.png">
</div>
<div class="title">Figure 1. Abstract view of a neural network</div>
</div>
<div class="paragraph">
<p>So basically you present to your network some input data (images, numbers, text)
and it generates an output (images, numbers, text) by computing:</p>
</div>
<div class="paragraph text-center">
<p>\(Y = f(X,W)\)</p>
</div>
<div class="paragraph">
<p>Note that the dimensions of \(X\),\(Y\) and \(W\) are generally different.
For example for an image classification task:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>\(X\) will be the dimensions of the image (example 128x128)</p>
</li>
<li>
<p>\(Y\) will be the number of class (example 10) you want to detect</p>
</li>
<li>
<p>\(W\) can be very large like millions of coefficients</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The \(f()\) function is a kind of generic one defined by your network
architecture and depends on:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>type of neuron</p>
</li>
<li>
<p>number of neurons</p>
</li>
<li>
<p>how neurons interconnect each other</p>
</li>
<li>
<p>&#8230;&#8203;</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Its behavior is driven by the weights. The weights are the parameters you have to
tune so that \(f()\) does something interesting i.e. produces relevant output
given the input and not just garbage.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_what_is_training">What is training ?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The goal of training is to find the value of the weights so that the network
generates correct output.</p>
</div>
<div class="paragraph">
<p>On supervised learning you present examples of input data and expected output.
At the beginning the network will generate outputs far different from the
expected ones.</p>
</div>
<div class="paragraph">
<p>The difference between output and expected output is used to update the weights
so that the error decreased.</p>
</div>
<div class="paragraph">
<p>After many example submission and weights optimization cycle, you hope your
network works correctly and can predict correctly new output with unseen data.</p>
</div>
<div class="paragraph">
<p>So basically network training is a function fitting task.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_how_do_you_train">How do you train ?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>OK first we need to measure the error between an expected output and current
output of the network.</p>
</div>
<div class="paragraph">
<p>This is called a loss function and it is added at the end of the network like this:</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://elinep.github.io/blog/images/2017-06-12-neural_networks_training_basics/fig2.png" alt="fig2.png">
</div>
<div class="title">Figure 2. Loss function to compute error</div>
</div>
<div class="paragraph">
<p>The loss function should have several properties:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>its input are the output of the network and the output from the training data</p>
</li>
<li>
<p>its output is a scalar</p>
</li>
<li>
<p>it should measure properly the error : the more \(Y\) and \(\tilde{Y}\)
are close, the less the error must be</p>
</li>
<li>
<p>it must be differentiable</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>There are several ad-hoc loss function that we can use depending on our application.
A very simple one - the \(L2\) norm - consists in summing the square error on
each output dimensions:</p>
</div>
<div class="paragraph text-center">
<p>\(L(Y,\tilde{Y}) = \sum{(Y_i-\tilde{Y_i})^2}\)</p>
</div>
<div class="paragraph">
<p>The training consists of finding the weights of the network that minimize the
error. In math language, we perform:</p>
</div>
<div class="paragraph text-center">
<p>\(\underset{W}{\arg\min{}} L(Y,\tilde{Y})\)</p>
</div>
<div class="paragraph">
<p>It’s not a straight forward job. The problem is that the loss function is on top
of the network which is a giant non linear function. We can’t compute the minimum
right away from the data, network and loss function.</p>
</div>
<div class="paragraph">
<p>How do we do then ? The answer is <strong>gradient descent</strong>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_gradient_descent">Gradient descent</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Gradient descent is widely use to optimize function parameter. The basic idea is
to test how the error evolves when slightly modifying each parameter.</p>
</div>
<div class="sect2">
<h3 id="_analogy">Analogy</h3>
<div class="paragraph">
<p>The famous analogy is a blind folded walker in the mountain looking for the valley.</p>
</div>
<div class="paragraph">
<p>One possible strategy is to sense the slope by making tiny steps:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>make a tiny step forward (X-axis) and sense if you’re going uphill or downhill
(i.e. watch how you move on Z-axis). Go back to your position</p>
</li>
<li>
<p>make a tiny side step (Y-axis) and sense if you’re going uphill or downhill
(i.e. watch how you move on Z-axis). Go back to your position</p>
</li>
<li>
<p>Thanks to this two steps, you know how the slope is oriented and you can make
a step in the correct direction, generally a mixed of X and Y</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>After several iterations you likely end in the valley, or at least in a local
valley.</p>
</div>
<div class="paragraph">
<p>The analogy points are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>the altitude (along Z-axis) is the error</p>
</li>
<li>
<p>your position (along X-axis and Y-axis) are the 2 parameters you want to
optimize to find the minimum error</p>
</li>
<li>
<p>the tiny steps to sense the slope is the gradient</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_simple_example">Simple example</h3>
<div class="paragraph">
<p>Let’s formalize a bit this strategy with an example.
Imagine you have the following loss function:</p>
</div>
<div class="paragraph text-center">
<p>\(L(w_1,w_2) = (w_1+2)^2+4(w_2-1)^2+3\)</p>
</div>
<div class="paragraph">
<p>To perform gradient descent, we need to compute the gradient of this loss function.
The gradient is just the set of partial derivative of the loss function for the
weight parameters.</p>
</div>
<div class="paragraph text-center">
<p>\(
\nabla L(w_1,w_2) =
\begin{bmatrix}
\frac{\partial L(w_1,w_2)}{\partial w_1}
\\
\frac{\partial L(w_1,w_2)}{\partial w_2}
\end{bmatrix}\)</p>
</div>
<div class="paragraph">
<p>The partial derivatives allows us to evaluate the slope along each axis at any
given position. Our simple loss function is easy to derivate:</p>
</div>
<div class="paragraph text-center">
<p>\(
\nabla L(w_1,w_2) =
\begin{bmatrix}
\frac{\partial L(w_1,w_2)}{\partial w_1}
\\
\frac{\partial L(w_1,w_2)}{\partial w_2}
\end{bmatrix}
=
\begin{bmatrix}
2w_1+4
\\
8w_2-8
\end{bmatrix}\)</p>
</div>
<div class="paragraph">
<p>Now that we can compute the slope we can run the algorithm:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>evaluate the local gradient at the current position</p>
</li>
<li>
<p>update the position by making step proportional to the local gradient</p>
</li>
<li>
<p>go back to 1.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Let’s do the first iteration:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>we start at a random position \(w_1=-5, w_2=5\)</p>
</li>
<li>
<p>we evaluate the gradient at this position
\(
\nabla L(-5,5) =
\begin{bmatrix}
\frac{\partial L(-5,5)}{\partial -5}
\\
\frac{\partial L(-5,5)}{\partial 5}
\end{bmatrix}
=
\begin{bmatrix}
2\times-5+4
\\
8\times5-8
\end{bmatrix}
=
\begin{bmatrix}
-6
\\
32
\end{bmatrix}\)
which means that locally the error decreases a bit when \(w_1\) increases,
and the error increases a lot when \(w_2\) increases.
Therefore to go downhill we have to increase \(w_1\) and decrease \(w_2\)</p>
</li>
<li>
<p>Update current position to go downhill
\(
\begin{bmatrix}
w_1
\\
w_2
\end{bmatrix}
=
\begin{bmatrix}
w_1
\\
w_2
\end{bmatrix}
-
\lambda
\nabla L(w_1,w_2)
=
\begin{bmatrix}
-5+6\lambda
\\
5-32\lambda
\end{bmatrix}\)
where \(\lambda\) is a scalar parameter to control the size of your step</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Here is graphically what happens for few iterations with \(\lambda=0.04\):</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://elinep.github.io/blog/images/2017-06-12-neural_networks_training_basics/fig_gradient_descent.png" alt="fig gradient descent.png">
</div>
<div class="title">Figure 3. Gradient descent iterations for a simple function - The red dot, is the initial position. The arrows represents the evaluation of the current gradient (the slope on each axis). The surface is the loss function.</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
When to stop the algorithm? Generally for neural networks training,
you run it a fix number of times. It is one of your hyperparameter. In the same
time you monitor that your error behave correctly (i.e. decreases)
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>We presented the basic gradient descent algorithm. It has a disadvantage of being
very slow (you need a lot of iteration to find the best position). There are
clever way to update the position but they all rely on the local gradient
evaluation.</p>
</div>
<div class="paragraph">
<p>OK so to train a neural network, we need to compute the partial derivative of
the loss:</p>
</div>
<div class="paragraph text-center">
<p>\(\frac{\partial L(f(X,W),\tilde{Y})}{\partial{W_i}}\)</p>
</div>
<div class="paragraph">
<p>The problem is that it’s not straightforward as \(f()\) can be very complex.
We can’t compute it by hand as we did with our little example. We could evaluate
numerically the gradient by running the network with tiny variation on each weight
at a time and monitor the error but it has drawbacks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>it is an approximation</p>
</li>
<li>
<p>you need to run the network (i.e. evaluate ) as many times as the number of
weights just for one gradient descent iteration which is impractical.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The solution is the <strong>backward propagation</strong>.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_backward_propagation">Backward propagation</h2>
<div class="sectionbody">
<div class="paragraph">
<p>So far we abstracted the neural network with the function \(Y = f(X,W)\).
Before introducing back propagation, let’s see how this function looks like.</p>
</div>
<div class="sect2">
<h3 id="_neural_networks_under_the_hood">Neural networks, under the hood</h3>
<div class="paragraph">
<p>A neural network is a composition of elementary functions called neurons.
The neuron transformation is quite basic. It combines the input and generate a
scalar output. There are several variants but here is a typical one:</p>
</div>
<div class="paragraph text-center">
<p>\(Y = f_{neuron}(X,W,B) = \max(0,\sum\limits_{i=0}^{N}{(X_i \cdot W_i)}+B)\)</p>
</div>
<div class="paragraph text-center">
<p>with \(Y \in \mathbb{R}\), \(X \in \mathbb{R}^N\), \(W \in \mathbb{R}^{N}\)
and \(B \in \mathbb{R}\)</p>
</div>
<div class="paragraph">
<p>This function simply computes a weighted sum of the input data,
adds a bias and saturates the result with \(\max()\). The \(\max()\) part
is called the activation function.
There are different possible activation functions.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The bias like the weights is a parameter that needs to be trained.
That’s why \(B\) is often omitted and included in \(W\) and we simply write: \(Y = f_{neuron}(X,W)\)
with \(W \in \mathbb{R}^{N+1}\)
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>By chaining several layers of neurons, you obtain a neural network:</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://elinep.github.io/blog/images/2017-06-12-neural_networks_training_basics/fig3.png" alt="fig3.png">
</div>
<div class="title">Figure 4. A neural network</div>
</div>
<div class="paragraph">
<p>Backward propagation is a recursive method to evaluate the gradient of
the loss function (i.e. \(\frac{\partial L(Y,\tilde{Y})}{\partial W_i}\))
by considering one simple neuron at a time.</p>
</div>
</div>
<div class="sect2">
<h3 id="_chain_rule">Chain rule</h3>
<div class="paragraph">
<p>Backward propagation relies on the chain rule.
The chain rule is a method to compute the derivative of the composition of
two functions.</p>
</div>
<div class="paragraph">
<p>Consider two functions \(y_f=f(x_f)\), \(y_g=g(x_g)\) and compose them
to form a third one:</p>
</div>
<div class="paragraph text-center">
<p>\(y=h(x)=g(f(x))\)</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://elinep.github.io/blog/images/2017-06-12-neural_networks_training_basics/fig4.png" alt="fig4.png">
</div>
<div class="title">Figure 5. Composition of two functions</div>
</div>
<div class="paragraph">
<p>Chain rule states:</p>
</div>
<div class="paragraph text-center">
<p>\(\frac{df}{dx}(a) = \frac{dg}{df}(f(a)) \cdot \frac{df}{dx}(a)\)</p>
</div>
<div class="paragraph">
<p>In other word, to derivate a composition of function we simply multiply the
derivative of the underlying functions where they are individually evaluated.</p>
</div>
<div class="paragraph">
<p>Let’s see with an example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>\(f(x)=3x+1\)</p>
</li>
<li>
<p>\(g(f)=f^2\)</p>
</li>
<li>
<p>\(h(x)=g(f(x))=(3x+1)^2=9x^2+6x+1\)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>By evaluating the derivative of \(h\) on point \(a\) directly we find:</p>
</div>
<div class="paragraph text-center">
<p>\(\frac{dh}{dx}(a) = 18a+6\)</p>
</div>
<div class="paragraph">
<p>With the chain rule method we find:</p>
</div>
<div class="paragraph text-center">
<p>\(\frac{dg}{df}=2f\), \(\frac{df}{dx}=3\)</p>
</div>
<div class="paragraph">
<p>so,</p>
</div>
<div class="paragraph text-center">
<p>\(\frac{dg}{df}(f(a))=2f(a)=2(3a+1)\), \(\frac{df}{dx}(a)=3\)</p>
</div>
<div class="paragraph">
<p>putting all together,</p>
</div>
<div class="paragraph text-center">
<p>\(\frac{dh}{dx}(a)=\frac{dg}{df}(f(a)) \cdot \frac{df}{dx}(a) = (6a+2) \cdot 3 = 18a+6\)
as expected</p>
</div>
<div class="paragraph">
<p>From \(f\) point of view, it means that to compute the derivative regarding \(x\)
for a particular value \(a\), we need:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>\(\frac{df}{dx}(a)\), the local derivative</p>
</li>
<li>
<p>\(\frac{dg}{df}(f(a))\) which is the evaluation of the local derivative of
the next « block » i.e. the \(g\) function</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_chain_rule_applied_to_a_network">Chain rule applied to a network</h3>
<div class="paragraph">
<p>The idea of back propagation is to apply chain rule on the loss function
\(L(Y,\tilde{Y})\).
For each function of the network (neuron, loss) we will evaluate the gradient of
the output regarding the input that are influenced by the weights of the networks.</p>
</div>
<div class="sect3">
<h4 id="_gradient_propagation_in_the_loss_function">Gradient propagation in the loss function</h4>
<div class="paragraph">
<p>We start by evaluating the gradient of the error on the loss function.
The loss function depends on two variable:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>the network output \(Y\) which obviously depends on the weights \(W\).
The gradient \(G_Y\) must be evaluated</p>
</li>
<li>
<p>the expected output \(\tilde{Y}\) from the training data which does not depend
on the weight. There is no need to compute \(G_{\tilde{Y}}\)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For the loss function we introduced earlier we have:</p>
</div>
<div class="paragraph text-center">
<p>\(G_{Y_i} = \frac{\partial L(Y,\tilde{Y})}{\partial Y_i}
= \frac{\partial \sum{(Y_i-\tilde{Y_i})^2} }{\partial Y_i}
= 2(Y_i - \tilde{Y}_i)\)</p>
</div>
<div class="paragraph">
<p>Once the gradient has been evaluated in the loss function, it can flows in the
neurons. We transmit \(G_Y\) to the last neurons of the network which are placed
right before the loss function.
These last neuron will also compute gradient and transmit it to their ancestors
and so on until we reach the first neurons of the network.</p>
</div>
</div>
<div class="sect3">
<h4 id="_gradient_propagation_in_a_neuron">Gradient propagation in a neuron</h4>
<div class="paragraph">
<p>Let&#8217;s focus on one neuron and assume we receive the gradient \(G_Y\) from the
next neurons (i.e. all the neurons that use our output as input) or from the loss
function when we are one of the last neurons.</p>
</div>
<div class="paragraph">
<p>Thanks to the chain rule we compute the following gradients:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>the weight gradient \(G_{W_i} = G_Y \cdot \frac{\partial F_{neuron}(X,W)}{\partial W_i}\)
senses the influence of the neuron&#8217;s weights on the final error. \(G_{W_i}\)
will be used to update the current neuron weights</p>
</li>
<li>
<p>the bias gradient \(G_{B} = G_Y \cdot \frac{\partial F_{neuron}(X,W)}{\partial B}\)
for the same reason. \(G_{B}\) will be used to update the current neuron bias</p>
</li>
<li>
<p>the input gradient \(G_{X_i} = G_Y \cdot \frac{\partial F_{neuron}(X,W)}{\partial X_i}\)
senses the influence of the current neuron input on the final error. \(G_{X_i}\)
will tell to the previous neuron how it should modify its output so that our
output help to decrease the error.</p>
</li>
</ul>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://elinep.github.io/blog/images/2017-06-12-neural_networks_training_basics/fig5.png" alt="fig5.png">
</div>
<div class="title">Figure 6. Gradient flow inside a neuron</div>
</div>
<div class="paragraph">
<p>With the neuron definition we have seen, we obtain:</p>
</div>
<div class="paragraph text-center">
<p>\({F_{neuron}(X,W,B) = \max(0,\sum\limits_{i=0}^{N}{(X_i \cdot W_i)}+B)} = Y\)</p>
</div>
<div class="paragraph text-center">
<p>\(G_{W_i} = G_Y \cdot \frac{\partial F_{neuron}(X,W,B)}{\partial W_i}
=G_Y \cdot
\Big\{
\begin{matrix}
X_i &amp; Y&gt;0 \\
0 &amp; Y \leqslant 0
\end{matrix}\)</p>
</div>
<div class="paragraph text-center">
<p>\(G_{B} = G_Y \cdot \frac{\partial F_{neuron}(X,W,B)}{\partial W_B}
=G_Y \cdot
\Big\{
\begin{matrix}
1 &amp; Y&gt;0 \\
0 &amp; Y \leqslant 0
\end{matrix}\)</p>
</div>
<div class="paragraph text-center">
<p>\(G_{X_i} = G_Y \cdot \frac{\partial F_{neuron}(X,W,B)}{\partial W_i}
=G_Y \cdot
\Big\{
\begin{matrix}
W_i &amp; Y&gt;0 \\
0 &amp; Y \leqslant 0
\end{matrix}\)</p>
</div>
<div class="paragraph">
<p>\(G_{X_i}\) are transmitted to previous neurons while \(G_{W_i}\) and
\(G_{B}\) are used by the gradient descent algorithm to update parameters
\(W\) and \(B\) for the current neuron.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Most of the time a neuron output is linked to multiple neurons.
Therefore during back propagation a neuron receive several \(G_{Y_i}\) gradients.
The gradient used for back propagation is simply the sum of all the incoming
gradients \(G_Y = \sum{G_{Y_i}}\) (see <a href="https://en.wikipedia.org/wiki/Chain_rule#Higher_dimensions" class="bare">https://en.wikipedia.org/wiki/Chain_rule#Higher_dimensions</a>).
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_training_process_overview">Training process overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>We have all the tools required to train a network. Let&#8217;s summarize:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>we set up a network by connecting multiple neurons</p>
</li>
<li>
<p>we prepare training data: input \(X\) and ground truth associated output
\(Y\)</p>
</li>
<li>
<p>we choose a loss function and add it at the end of the network</p>
</li>
<li>
<p>we pick up an example and run it through the network and the loss function
(<strong>forward propagation</strong>)</p>
</li>
<li>
<p>we compute gradient from the loss function to the first neurons of the network
(<strong>backward propagation</strong>)</p>
</li>
<li>
<p>thanks to the weight and bias gradients, we make a tiny update of the neuron&#8217;s
weights and bias (<strong>gradient descent</strong>)</p>
</li>
<li>
<p>we go back to <strong>4</strong> and loop for many iterations.</p>
</li>
</ol>
</div>
</div>
</div>]]></description><link>https://elinep.github.io/blog/2017/06/12/neural_networks_training_basics.html</link><guid isPermaLink="true">https://elinep.github.io/blog/2017/06/12/neural_networks_training_basics.html</guid><category><![CDATA[Blog]]></category><category><![CDATA[Neural network]]></category><pubDate>Mon, 12 Jun 2017 00:00:00 GMT</pubDate></item></channel></rss>