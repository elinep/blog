<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <title>Neural Networks from scratch</title>
    <meta name="description" content="" />
    <link href="//fonts.googleapis.com/css?family=Noto+Sans:300,400,700" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Noto+Serif:400,700,400italic" rel="stylesheet" type="text/css">
    <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
    <link href="//elinep.github.io/blog/themes/saga/assets/css/style.css?v=1497426826885" rel="stylesheet" type="text/css">
    <link href="//elinep.github.io/blog/themes/saga/assets/css/animate.min.css?v=1497426826885" rel="stylesheet" type="text/css">
    <link href="https://elinep.github.io/blog/favicon.ico" rel="shortcut icon">
    <link rel="canonical" href="https://elinep.github.io/blog/2017/06/13/neural_network_from_scratch.html" />
    <meta name="referrer" content="origin" />
    
    <meta property="og:site_name" content="" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Neural Networks from scratch" />
    <meta property="og:description" content="In the previous post we saw the theory about neural network training. It&amp;#8217;s time to practice by writing a basic implementation from scratch in python. The code is available here. Objectives Few goals for our tiny neural network framework : the network architecture must be easy to define (number" />
    <meta property="og:url" content="https://elinep.github.io/blog/2017/06/13/neural_network_from_scratch.html" />
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Neural Networks from scratch" />
    <meta name="twitter:description" content="In the previous post we saw the theory about neural network training. It&amp;#8217;s time to practice by writing a basic implementation from scratch in python. The code is available here. Objectives Few goals for our tiny neural network framework : the network architecture must be easy to define (number" />
    <meta name="twitter:url" content="https://elinep.github.io/blog/2017/06/13/neural_network_from_scratch.html" />
    
    <script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "Article",
    "publisher": "",
    "author": {
        "@type": "Person",
        "name": "elinep",
        "image": "https://avatars0.githubusercontent.com/u/29272214?v=3",
        "url": "https://elinep.github.io/blog/author/elinep/"
    },
    "headline": "Neural Networks from scratch",
    "url": "https://elinep.github.io/blog/2017/06/13/neural_network_from_scratch.html",
    "description": "In the previous post we saw the theory about neural network training. It&amp;#8217;s time to practice by writing a basic implementation from scratch in python. The code is available here. Objectives Few goals for our tiny neural network framework : the network architecture must be easy to define (number"
}
    </script>

    <meta name="generator" content="HubPress" />
    <link rel="alternate" type="application/rss+xml" title="" href="https://elinep.github.io/blog/rss/" />
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.10.0/styles/atom-one-dark.min.css">
</head>
<body class="post-template">
    <header id="header" class="animated fadeIn">
    <div class="header-background">
    <section class="blog-content">
        <a id="site-url" class="blog-title" href="https://elinep.github.io/blog"></a>
        <span class="blog-description"></span>
        <nav class="links fadeIn animated">
            <ul>
                <li class="rss"><a title="RSS Feed" href="/rss/"><i class="fa fa-fw fa-rss-square"></i></a></li>
        
            </ul>
        </nav>
    </section>
    <section class="header-content">
        <h1 class="post-title animated fadeInUp">Neural Networks from scratch</h1>
        <span class="post-data"><span class="date animated fadeInUp"><i class="fa fa-clock-o"></i> <time class="timesince date" data-timesince="1497426826" datetime="2017-06-14T09:53" title="14 June 2017">a few seconds ago</time></span>
            
            <span class="author animated fadeInUp"><i class="fa fa-user"></i> <a href="https://elinep.github.io/blog/author/elinep/">elinep</a></span></span>
    </section>
    </div>
</header>
<main id="main" class="content">
    <article itemtype="http://schema.org/BlogPosting" class="animated fadeIn content post post">
        <section class="post-content">
            <div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In the <a href="https://elinep.github.io/blog/2017/06/12/neural_networks_training_basics.html">previous post</a>
we saw the theory about neural network training. It&#8217;s time to practice by writing
a basic implementation from scratch in python. The code is available
<a href="https://github.com/elinep/nn_from_scratch/blob/master/scratch_nn.py">here</a>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_objectives">Objectives</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Few goals for our tiny neural network framework :</p>
</div>
<div class="ulist">
<ul>
<li>
<p>the network architecture must be easy to define (number of layers, neuron
per layers, &#8230;&#8203;)</p>
</li>
<li>
<p>we will focus on fully connected network</p>
</li>
<li>
<p>it must be trainable</p>
</li>
<li>
<p>no use of specialized package like tensorflow</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To test it, we will train the network to approximate a function. I arbitrarily
choose the next one:</p>
</div>
<div class="paragraph text-center">
<p>\(y = f(x_0,x_1) = x_0 x_1 + 2x_0 + 1\)</p>
</div>
<div class="paragraph text-center">
<p>\((x_0,x_1) \in \mathbb{R}^2\) and \(y \in \mathbb{R}\)</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://elinep.github.io/blog/images/2017-06-13-neural_network_from_scratch/fig1.png" alt="fig1.png">
</div>
<div class="title">Figure 1. Our objective function</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_code">Code</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_block_class">Block class</h3>
<div class="sect3">
<h4 id="_abstractblock_class">AbstractBlock class</h4>
<div class="paragraph">
<p>We start by defining an abstract class which will be the base class for all the
nodes (neurons, activation function, loss function) in the network.</p>
</div>
<div class="paragraph">
<p>This class defines 3 methods:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>forward()</strong>, receives input data and return an output, used for <strong>forward
propagation</strong></p>
</li>
<li>
<p><strong>backward()</strong> receives output gradient and return input gradient, used for
<strong>backward propagation</strong></p>
</li>
<li>
<p><strong>update()</strong> called during <strong>weights update</strong>. This function is optional because
blocks like loss/activation functions do not have weights.</p>
</li>
</ul>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">class AbstractBlock(object):
    '''
    Abstract class of a block
    a block has multiple input, one output.
    a block must have forward() and backward() methods
    update() method is optional
    '''
    def __init__(self, input_dim):
        self.input_dim = input_dim
        # input data
        self.x = np.zeros(input_dim)
        # output data
        self.y = 0
        # gradient data
        self.grad_x = np.zeros(input_dim)

    '''
    forward pass
    input_data is a vector, each element corresponds to a neuron input
    '''
    def forward(self, input_data):
        raise NotImplementedError('forward not implemented')

    '''
    backward pass
    output_gradient is a scalar
    '''
    def backward(self, output_gradient):
        raise NotImplementedError('backward not implemented')

    '''
    trigger weight update
    '''
    def update(self, learn_rate):
        pass</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
forward() and backward() do not have default implementation as we don&#8217;t know
yet what an AbstractBlock computes.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_neuron_class">Neuron class</h4>
<div class="paragraph">
<p>Now we define the <strong>Neuron</strong> class which inherits from <strong>AbstractBlock</strong> and implements
the neuron transformation:</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">class Neuron(AbstractBlock):
    '''
    Neuron class computing a weighted sum of its input
    '''
    def __init__(self, input_dim):
        super(Neuron, self).__init__(input_dim)
        # initialize random weight
        self.weight = np.random.normal(loc=0.0, scale=0.1, size=(input_dim))
        # initialize bias
        self.bias = 0.0
        # init weight and bias gradient
        self.grad_weight = np.zeros(input_dim)
        self.grad_bias = 0.0

    def forward(self, input_data):
        # save input data
        self.x = input_data
        # compute output
        self.y = np.dot(self.x, self.weight) + self.bias
        return self.y

    def backward(self, output_gradient):
        self.grad_bias = output_gradient
        self.grad_weight = self.x * output_gradient
        self.grad_x = self.weight * output_gradient
        return self.grad_x

    def update(self, learn_rate):
        self.weight += -learn_rate * self.grad_weight
        self.bias += -learn_rate * self.grad_bias</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Neuron</strong> has some extra initialization compared to <strong>AbstractBlock</strong> since it has
weight and bias parameters. Weights are randomly initialized while bias is set to
0</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">def __init__(self, input_dim):
    super(Neuron, self).__init__(input_dim)
    # initialize random weight
    self.weight = np.random.normal(loc=0.0, scale=0.1, size=(input_dim))
    # initialize bias
    self.bias = 0.0
    # init weight and bias gradient
    self.grad_weight = np.zeros(input_dim)
    self.grad_bias = 0.0</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>forward()</strong> implements the neuron definition:</p>
</div>
<div class="paragraph text-center">
<p>\(Y = f_{neuron}(X,W,B) = \sum\limits_{i=0}^{N}{(X_i \cdot W_i)}+B\)</p>
</div>
<div class="paragraph">
<p><strong>backward()</strong> compute the partial derivative with respect to the parameters (weight, bias)
and the input data (x) and multiply the result by the output gradient (chain rule):</p>
</div>
<div class="paragraph text-center">
<p>\(G_{W_i} = G_Y \cdot \frac{\partial F_{neuron}(X,W,B)}{\partial W_i}
=G_Y \cdot X_i\)</p>
</div>
<div class="paragraph text-center">
<p>\(G_{B} = G_Y \cdot \frac{\partial F_{neuron}(X,W,B)}{\partial W_B}
=G_Y \cdot 1\)</p>
</div>
<div class="paragraph text-center">
<p>\(G_{X_i} = G_Y \cdot \frac{\partial F_{neuron}(X,W,B)}{\partial W_i}
=G_Y \cdot W_i\)</p>
</div>
<div class="paragraph">
<p><strong>update()</strong> implements the gradient descent by incrementing the neuron parameter
with a fraction of the gradient:</p>
</div>
<div class="paragraph text-center">
<p>\(W_i = W_i - \lambda G_{W_i}\)</p>
</div>
<div class="paragraph text-center">
<p>\(B = B - \lambda G_{B}\)</p>
</div>
</div>
<div class="sect3">
<h4 id="_activation_class">Activation class</h4>
<div class="paragraph">
<p>The activation function is placed right after a neuron. It is a
\(\mathbb{R} \to \mathbb{R}\) as opposite to a neuron which is
\(\mathbb{R}^N \to \mathbb{R}\). It doesn&#8217;t have any parameters. Here we implement
the popular <strong>relu</strong>:</p>
</div>
<div class="paragraph text-center">
<p>\(Y = f_{relu}(X) = \max(0,X)\)</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">class Relu(AbstractBlock):
    '''
    Relu activation function block
    '''
    def __init__(self):
        super(Relu, self).__init__(1)

    def forward(self, input_data):
        # save input data
        self.x = input_data
        # compute output
        self.y = max(0.0, self.x)
        return self.y

    def backward(self, output_gradient):
        if self.x &gt; 0:
            self.grad_x = output_gradient
        else:
            self.grad_x = 0.0
        return self.grad_x</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>backward()</strong> is straight forward to implement. The function varies only with its
scalar input:</p>
</div>
<div class="paragraph text-center">
<p>\(G_X = G_Y \cdot \frac{\partial f_{relu}(X)}{\partial X}
=G_Y \cdot
\Big\{
\begin{matrix}
1 &amp; Y&gt;0 \\
0 &amp; Y \leqslant 0
\end{matrix}\)</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
strictly speaking the \(f_{relu}(X)\) can not be derived for \(X=0\)
In practice it is not annoying as it affects only one single point and we arbitrarily
choose to extend the derivative from \(X&lt;0\) or \(X&gt;0\)
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p><strong>update()</strong> class is not redefined as there are no weights to optimize in an activation
function</p>
</div>
</div>
<div class="sect3">
<h4 id="_loss_class">Loss class</h4>
<div class="paragraph">
<p>A loss function computes the error between the output of the network and the expected
output from the training data. We need a way to provide the expected data to a loss <strong>block</strong>.
We define an <strong>AbstractLoss</strong> class for this purpose</p>
</div>
<div class="sect4">
<h5 id="_abstractloss_class">AbstractLoss class</h5>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">class AbstractLoss(AbstractBlock):
    '''
    Abstract class of a loss function
    A loss function is a "block" with a setExpectedData() method to set
    the ground truth result of a neural network
    '''
    def __init__(self, input_dim):
        super(AbstractLoss, self).__init__(input_dim)
        self.expected_data = np.zeros(input_dim)

    def setExpectedData(self, expected_data):
        self.expected_data = expected_data</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>AbstractLoss</strong> only adds a new method <strong>setExpectedData()</strong> to the <strong>AbstractBlock</strong>.
<strong>setExpectedData()</strong> is used to pass the ground truth output from the training data.
The expected data are stored to be used later by the <strong>forward()</strong> method.</p>
</div>
</div>
<div class="sect4">
<h5 id="_lossl2_class">LossL2 class</h5>
<div class="paragraph">
<p>Now we implement a real loss function, the \(L2\) norm:</p>
</div>
<div class="paragraph text-center">
<p>\(L(Y,\tilde{Y}) = \sum{(Y_i-\tilde{Y_i})^2}\)</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">class LossL2(AbstractLoss):
    '''
    A loss function that compute the L2 norm between its input_data and
    ground truth data
    '''
    def __init__(self, input_dim):
        super(LossL2, self).__init__(input_dim)

    def forward(self, input_data):
        # save input data
        self.x = input_data
        # compute L2 distance between input data and expected input data
        self.y = np.sum(np.square(self.x - self.expected_data))
        return self.y

    def backward(self):
        self.grad_x = 2 * (self.x - self.expected_data)
        return self.grad_x</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>backward()</strong> takes no argument as we compute the gradient with respect to the
error. The loss function is the first stage for
<strong>backward propagation</strong>. We only compute the gradient with respect to \(Y\) as
\(\tilde{Y}\) do not varies with the weights of the network.</p>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_network_class">Network class</h3>
<div class="paragraph">
<p>We have all the elementary blocks we need. Let&#8217;s write a new class to manage them.
The <strong>Network</strong> class will allow us to build/train and run a network.</p>
</div>
</div>
<div class="sect2">
<h3 id="_building_network">building network</h3>
<div class="paragraph">
<p>First let&#8217;s focus on the functions to build the network</p>
</div>
<div class="paragraph">
<p>When instantiating the <strong>Neuron</strong> we provide the dimensions of our input and output
data. These informations are used to determine the number of inputs per neuron of
the first layer and the number of neurons of the last layer.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">def __init__(self, input_dim, output_dim):
    # data dimensions
    self._input_dim = input_dim
    self._output_dim = output_dim
    # the neuron layer stack
    self._network = []
    # helper to know the number of input for the next neuron layer
    self._last_input_dim = input_dim
    # the loss function will be added later
    self._loss = None
    # current output of the network
    self._last_network_output = np.zeros(output_dim)</code></pre>
</div>
</div>
<div class="paragraph">
<p>We define a method to add a layer of neuron. This method takes an <strong>AbstractBlock</strong>
as first argument which defines the neuron transformation. The second argument is
the number of neurons for the layer.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">'''
Add a layer of neuron fully connected to the previous layer
'''
def addNeuronlayer(self, NeuronClassType, size):
    layer = {
        'data' : [ NeuronClassType(self._last_input_dim) for i in range(size)],
        'input_dim' : self._last_input_dim,
        'output_dim' : size,
        'type' : 'neuron'
    }
    self._network.append(layer)
    self._last_input_dim = size</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>addNeuronlayer()</strong> method adds a fully connected layer. Each neuron of the layer
considers all the output of the previous layer as input.
We instantiate <strong>size</strong> neurons, add some metadata and stack them into the
<strong>self._network</strong> array.</p>
</div>
<div class="paragraph">
<p>We write a similar method to add an activation layer.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">'''
Add an activation layer (one activation block per block of the previous layer)
'''
def addActivationLayer(self, ActivationClassType):
    layer = {
        'data' : [ ActivationClassType() for i in range(self._last_input_dim)],
        'input_dim' : self._last_input_dim,
        'output_dim' : self._last_input_dim,
        'type' : 'activation'
    }
    self._network.append(layer)
    pass</code></pre>
</div>
</div>
<div class="paragraph">
<p>Since activation function are placed after each neuron, there is no <strong>size</strong> argument.
We create as many <strong>ActivationClass</strong> as neurons in the previous layer.</p>
</div>
</div>
<div class="sect2">
<h3 id="_set_loss">set loss</h3>
<div class="paragraph">
<p>The user can set the loss function thanks to the next method</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">'''
Set the loss function for the optimization
'''
def setLoss(self, LossClass):
    self._loss = LossClass(self._output_dim)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_forward_backward_propagation_through_the_network">forward/backward propagation through the network</h3>
<div class="paragraph">
<p>Next method propagates data by iterating over layers and blocks. It returns the
network output.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">'''
Process input_data by the network
'''
def _forward(self, input_data):
    layer_input = input_data
    layer_i = 0
    for layer in self._network:
        # temporary layer output
        layer_output = np.zeros(layer['output_dim'])

        if layer['type'] == 'neuron':
            # feed each block of the layer with the input data
            b_i = 0
            for b in layer['data']:
                layer_output[b_i] = b.forward(layer_input)
                b_i += 1
        elif layer['type'] == 'activation':
            # one to one connection between input data and output data elements
            b_i = 0
            for b in layer['data']:
                layer_output[b_i] = b.forward(layer_input[b_i])
                b_i += 1
        else:
            raise ValueError('unknow layer type %s, can not run the network' % layer['type'])

        # current output is the input of the next layer
        layer_input = layer_output
        layer_i += 1

    return layer_output</code></pre>
</div>
</div>
<div class="paragraph">
<p>As we notice, there are two cases:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>layer of neuron, the entire output vector of the previous layer is fed to each
neurons since we are implementing a fully connected network.</p>
</li>
<li>
<p>layer of activation function, only the output of the matching previous layer neuron is fed
to the activation function</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The output of the current layer becomes the input of the next layer.</p>
</div>
<div class="paragraph">
<p>Similarly we have the backward propagation.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">'''
Back propagate through the entire network
'''
def _backward(self, output_grad):
    layer_output_grad = output_grad
    layer_i = 0
    for layer in reversed(self._network):
        # temporary layer input gradient
        layer_input_grad = np.zeros(layer['input_dim'])

        if layer['type'] == 'neuron':
            b_i = 0
            for b in layer['data']:
                layer_input_grad += b.backward(layer_output_grad[b_i])
                b_i += 1
        elif layer['type'] == 'activation':
            b_i = 0
            for b in layer['data']:
                layer_input_grad[b_i] = b.backward(layer_output_grad[b_i])
                b_i += 1
        else:
            raise ValueError('unknow layer type %s, can not run the network' % layer['type'])

        layer_output_grad = layer_input_grad
        layer_i += 1
    return</code></pre>
</div>
</div>
<div class="paragraph">
<p>There are also two cases:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>layer of neuron, we accumulate the gradient since the output of the previous layer is
connected to multiple neurons.</p>
</li>
<li>
<p>layer of activation function, we provide a scalar gradient and get a scalar gradient</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_training">Training</h3>
<div class="paragraph">
<p>The training iterates over a set of examples. Each example is forward propagated
through the network. The error is computed thanks to the loss block. We then back
propagate through the loss and the network. Finally we trigger a weight <strong>update()</strong>
for all <strong>block</strong> of the network. In the meantime we average the error over the whole
training set to monitor the training behavior. The full training set is presented
<strong>epoch</strong> times.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">'''
Train the network with examples
(X,Y) training data set (each row is an example)
'''
def train(self, X, Y, epoch, learn_rate):
    num_example = X.shape[1]
    loss_historic = np.zeros(epoch)
    for e in range(epoch):
        l_mean = 0.0
        for x, y in zip(X, Y):
            # do forward pass
            network_output = self._forward(x)
            # compute loss
            self._loss.setExpectedData(y)
            l_mean += self._loss.forward(network_output)
            # back propagate
            l_grad = self._loss.backward()
            self._backward(l_grad)
            # update weight
            self._update(learn_rate)
        loss_historic[e] = l_mean / num_example
        print("epoch %d/%d average loss %f" % (e, epoch, loss_historic[e]))
    return loss_historic

    '''
    Trigger a weight update for every block of the network
    '''
    def _update(self, learn_rate):
        for layer in self._network:
            for b in layer['data']:
                b.update(learn_rate)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_running">Running</h3>
<div class="paragraph">
<p>Finally a method for forward propagation only to use our network once trained.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">'''
Process a bunch of input data
'''
def run(self, input_data):
    input_data = input_data.reshape((-1, self._input_dim))
    y = np.zeros((input_data.shape[0], self._output_dim))
    i = 0
    for x in input_data:
        y[i, :] = self._forward(x)
        i += 1
    return y</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_main">Main</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The main() function prepares the training data, sets up the network, trains it
and displays the results.</p>
</div>
<div class="sect2">
<h3 id="_training_data">Training data</h3>
<div class="paragraph">
<p>We define a lambda objective function as defined in the introduction. We generate
random input data and process them with our objective function.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python"># the objective function the network has to mimic
function_to_learn = lambda x0, x1: np.add(np.multiply(x0, x1), 2 * x0) + 1

x_range = 10
# generate random input data
X_train = np.random.uniform(-x_range, x_range, (NUM_EXAMPLES, 2))
# generate matching output data
Y_train = function_to_learn(X_train[:, 0], X_train[:, 1])</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_build_model">Build model</h3>
<div class="paragraph">
<p>We instantiate a network and stack neuron layers and activation layers. Each layer
has <strong>NUM_HIDDEN_NODES</strong> neurons and there are <strong>NUM_HIDDEN_LAYER</strong> layers. We end with
a single neuron which is the output of the network.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">model = Network(2, 1)
for h in range(NUM_HIDDEN_LAYER):
    model.addNeuronlayer(Neuron, NUM_HIDDEN_NODES)
    model.addActivationLayer(Relu)
model.addNeuronlayer(Neuron, 1)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_trainning">Trainning</h3>
<div class="paragraph">
<p>Finally we set a loss function and start training.</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">model.setLoss(LossL2)
loss_historic = model.train(X_train, Y_train, NUM_EPOCHS, LEARNING_RATE)</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_results">Results</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The script displays two figures. First, an overlay of the objective function and
the network function. As we can see both overlay quite nicely.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://elinep.github.io/blog/images/2017-06-13-neural_network_from_scratch/fig2.png" alt="fig2.png">
</div>
<div class="title">Figure 2. Network vs Objective function</div>
</div>
<div class="paragraph">
<p>The error also decreases nicely:</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://elinep.github.io/blog/images/2017-06-13-neural_network_from_scratch/fig3.png" alt="fig3.png">
</div>
<div class="title">Figure 3. Error over time</div>
</div>
<div class="paragraph">
<p>In the main function we have all the hyperparameters we can play with:</p>
</div>
<div id="src-listing" class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">NUM_HIDDEN_NODES = 10
NUM_HIDDEN_LAYER = 2
NUM_EXAMPLES = 1000
NUM_EPOCHS = 200
LEARNING_RATE = 0.0001</code></pre>
</div>
</div>
</div>
</div>
        </section>

    
        <section class="post-comments">
          <div id="disqus_thread"></div>
          <script type="text/javascript">
          var disqus_shortname = 'elinep-github-hubpres'; // required: replace example with your forum shortname
          /* * * DON'T EDIT BELOW THIS LINE * * */
          (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
          })();
          </script>
          <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
          <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
        </section>
    
    </article>

</main>
    <footer class="animated fadeIn" id="footer">
        <section class="colophon">
          <section class="copyright">Copyright &copy; <span itemprop="copyrightHolder"></span>. <span rel="license">All Rights Reserved</span>.</section>
          <section class="poweredby">Published with <a class="icon-ghost" href="http://hubpress.io">HubPress</a></section>
        </section>
        <section class="bottom">
          <section class="attribution">
            <a href="http://github.com/Reedyn/Saga">Built with <i class="fa fa-heart"></i> and Free and Open-Source Software</a>.
          </section>
        </section>
    </footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js?v="></script> <script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.9.0/moment-with-locales.min.js?v="></script> <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.10.0/highlight.min.js?v="></script> 
      <script type="text/javascript">
        jQuery( document ).ready(function() {
          // change date with ago
          jQuery('ago.ago').each(function(){
            var element = jQuery(this).parent();
            element.html( moment(element.text()).fromNow());
          });
        });

        hljs.initHighlightingOnLoad();
      </script>
       
    <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
    <script src="//elinep.github.io/blog/themes/saga/assets/js/scripts.js?v=1497426826885"></script>
    
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-100931138-1', 'auto');
    ga('send', 'pageview');

    </script>
</body>
</html>
